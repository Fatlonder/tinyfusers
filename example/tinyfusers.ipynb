{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMQj4oa55D7B"
      },
      "outputs": [],
      "source": [
        "get_ipython().system('nvidia-smi')\n",
        "get_ipython().system('python -m pip install nvidia-cudnn-cu12')\n",
        "get_ipython().system('CUDNN_PATH=`python -m pip show nvidia-cudnn-cu12  | grep Location | cut -d\":\" -f2 | xargs`/nvidia/cudnn python -m pip install git+https://github.com/NVIDIA/cudnn-frontend.git')\n",
        "!python -m pip install cupy-cuda12x==12.3.0 pillow\n",
        "!git clone --depth 1 --branch v0.8.0 https://github.com/tinygrad/tinygrad\n",
        "!git clone https://github.com/Fatlonder/tinyfusers.git .\n",
        "\n",
        "%cd tinygrad\n",
        "!rm -r build\n",
        "!python -m pip install -e .\n",
        "%cd ..\n",
        "\n",
        "%cd tinyfusers\n",
        "!rm -r build\n",
        "!python -m  pip install -e .\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MmB_-X46-auZ"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, 'tinyfusers')\n",
        "\n",
        "import tinyfusers\n",
        "import importlib\n",
        "importlib.reload(tinyfusers)\n",
        "from IPython.display import display\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "from tinygrad import GlobalCounters, Tensor\n",
        "from tinygrad.nn.state import torch_load\n",
        "from tinygrad.helpers import Timing, Context, getenv, fetch\n",
        "from tinyfusers.variants.sd import StableDiffusion\n",
        "from tinyfusers.tokenizer.clip import ClipTokenizer\n",
        "from tinyfusers.storage.state import update_state\n",
        "from tinyfusers.storage.unpicker import load_weights\n",
        "import gc\n",
        "import ctypes\n",
        "\n",
        "libc = ctypes.CDLL(\"libc.so.6\")\n",
        "mempool = cp.get_default_memory_pool()\n",
        "pinned_mempool = cp.get_default_pinned_memory_pool()\n",
        "\n",
        "def model_to_fp16(model):\n",
        "  for l in get_state_dict(model).values():\n",
        "    l.replace(l.cast(dtypes.float16).realize())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YLC4Dyfq-eaY"
      },
      "outputs": [],
      "source": [
        "model = StableDiffusion()\n",
        "default_prompt = \"a horse sized cat eating a bagel\"\n",
        "args = {\"prompt\": default_prompt, \"steps\": 20, \"fp16\": True, \"out\": \"rendered.png\", \"noshow\": False, \"timing\": False, \"guidance\":7.5, \"seed\": 42}\n",
        "state_dictionary = torch_load(fetch('https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt', 'sd-v1-4.ckpt'))\n",
        "update_state(model, state_dictionary['state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSvvI1Ps-z_N"
      },
      "outputs": [],
      "source": [
        "# run through CLIP to get context\n",
        "tokenizer = ClipTokenizer()\n",
        "prompt = cp.array([tokenizer.encode(args['prompt'])])\n",
        "context = model.cond_stage_model.transformer.text_model(prompt)\n",
        "\n",
        "prompt = cp.array([tokenizer.encode(\"\")])\n",
        "unconditional_context = model.cond_stage_model.transformer.text_model(prompt)\n",
        "\n",
        "print(f\"CLIP context: {context.shape}, unconditional CLIP context: {unconditional_context.shape}\")\n",
        "del model.cond_stage_model\n",
        "\n",
        "timesteps = list(range(1, 1000, 1000//args['steps']))\n",
        "print(f\"running for {timesteps} timesteps\")\n",
        "alphas = model.alphas_cumprod[timesteps]\n",
        "alphas_prev = cp.concatenate((cp.array([1.0]), alphas[:-1])).astype(cp.float32)\n",
        "\n",
        "cur_stream = cp.cuda.get_current_stream()\n",
        "cur_stream.use()\n",
        "if args['seed'] is not None: Tensor._seed = args['seed']\n",
        "latent = Tensor.randn(1,4,64,64)\n",
        "latent = cp.asarray(latent.numpy())\n",
        "cur_stream.synchronize()\n",
        "cp.cuda.Device().synchronize()\n",
        "\n",
        "with Context(BEAM=getenv(\"LATEBEAM\")):\n",
        "  for index, timestep in (t:=tqdm(list(enumerate(timesteps))[::-1])):\n",
        "    GlobalCounters.reset()\n",
        "    t.set_description(\"%3d %3d\" % (index, timestep))\n",
        "    with Timing(\"step in \", enabled=args['timing'], on_exit=lambda _: f\", using {GlobalCounters.mem_used/1e9:.2f} GB\"):\n",
        "      tid = cp.array([index])\n",
        "      latent = model(unconditional_context, context, latent, cp.array([timestep]), alphas[tid], alphas_prev[tid], cp.array([args['guidance']]))\n",
        "x = model.decode(latent)\n",
        "print(x.shape)\n",
        "im = Image.fromarray(cp.asnumpy(x).astype(np.uint8, copy=False))\n",
        "print(f\"saving {args['out']}\")\n",
        "im.save(args['out'])\n",
        "display(im)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
