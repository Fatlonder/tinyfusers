{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMQj4oa55D7B"
      },
      "outputs": [],
      "source": [
        "get_ipython().system('nvidia-smi')\n",
        "get_ipython().system('python -m pip install nvidia-cudnn-cu12')\n",
        "get_ipython().system('CUDNN_PATH=`python -m pip show nvidia-cudnn-cu12  | grep Location | cut -d\":\" -f2 | xargs`/nvidia/cudnn python -m pip install git+https://github.com/NVIDIA/cudnn-frontend.git')\n",
        "!python -m pip install cupy-cuda12x pillow\n",
        "!git clone --depth 1 --branch v0.8.0 https://github.com/tinygrad/tinygrad\n",
        "!git clone https://github.com/Fatlonder/tinyfusers.git .\n",
        "\n",
        "%cd tinygrad\n",
        "!rm -r build\n",
        "!python -m pip install -e .\n",
        "%cd ..\n",
        "\n",
        "%cd tinyfusers\n",
        "!rm -r build\n",
        "!python -m  pip install -e .\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MmB_-X46-auZ"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, 'tinyfusers')\n",
        "\n",
        "import tinyfusers\n",
        "import importlib\n",
        "importlib.reload(tinyfusers)\n",
        "\n",
        "from IPython.display import display\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "from tinygrad import GlobalCounters, dtypes, Tensor\n",
        "from tinygrad.nn.state import torch_load, get_state_dict\n",
        "from tinygrad.helpers import Timing, Context, getenv, fetch\n",
        "from tinyfusers.variants.sd import StableDiffusion\n",
        "from tinyfusers.tokenizer.clip import ClipTokenizer\n",
        "from tinyfusers.storage.state import update_state\n",
        "import gc\n",
        "import ctypes\n",
        "\n",
        "libc = ctypes.CDLL(\"libc.so.6\")\n",
        "mempool = cp.get_default_memory_pool()\n",
        "pinned_mempool = cp.get_default_pinned_memory_pool()\n",
        "\n",
        "def model_to_fp16(model):\n",
        "  for l in get_state_dict(model).values():\n",
        "    l.replace(l.cast(dtypes.float16).realize())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YLC4Dyfq-eaY"
      },
      "outputs": [],
      "source": [
        "default_prompt = \"a horse sized cat eating a bagel\"\n",
        "args = {\"prompt\": default_prompt, \"steps\": 20, \"fp16\": True, \"out\": \"rendered.png\", \"noshow\": False, \"timing\": False, \"guidance\":7.5, \"seed\": 42}\n",
        "Tensor.no_grad = True\n",
        "model = StableDiffusion()\n",
        "state_dictionary = torch_load(fetch('https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt', 'sd-v1-4.ckpt'))\n",
        "update_state(model, state_dictionary['state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSvvI1Ps-z_N"
      },
      "outputs": [],
      "source": [
        "tokenizer = ClipTokenizer()\n",
        "prompt = cp.asarray([tokenizer.encode(args['prompt'])])\n",
        "context = model.cond_stage_model.transformer.text_model(prompt)\n",
        "print(\"got CLIP context\", context.shape)\n",
        "\n",
        "prompt = cp.asarray([tokenizer.encode(\"\")])\n",
        "unconditional_context = model.cond_stage_model.transformer.text_model(prompt)\n",
        "print(\"got unconditional CLIP context\", unconditional_context.shape)\n",
        "\n",
        "del model.cond_stage_model\n",
        "\n",
        "timesteps = list(range(1, 1000, 1000//args['steps']))\n",
        "print(f\"running for {timesteps} timesteps\")\n",
        "alphas = model.alphas_cumprod[timesteps]\n",
        "alphas_prev = cp.concatenate((cp.array([1.0]), alphas[:-1]))\n",
        "\n",
        "if args['seed'] is not None: cp.random.seed(seed=args['seed'])\n",
        "latent = cp.random.randn(1,4,64,64)\n",
        "\n",
        "with Context(BEAM=getenv(\"LATEBEAM\")):\n",
        "  for index, timestep in (t:=tqdm(list(enumerate(timesteps))[::-1])):\n",
        "    GlobalCounters.reset()\n",
        "    t.set_description(f\"({index}, {timestep})\")\n",
        "    with Timing(\"step in \", enabled=args['timing'], on_exit=lambda _: f\", using {GlobalCounters.mem_used/1e9:.2f} GB\"):\n",
        "      tid = cp.asarray([index])\n",
        "      latent = model(unconditional_context, context, latent, cp.asarray([timestep]), alphas[tid], alphas_prev[tid], cp.asarray([args['guidance']]))\n",
        "  gc.collect()\n",
        "  mempool.free_all_blocks()\n",
        "  pinned_mempool.free_all_blocks()\n",
        "  print(f\"Freed memory: {libc.malloc_trim(0)}\")\n",
        "x = model.decode(latent)\n",
        "im = Image.fromarray(x.numpy().astype(np.uint8, copy=False))\n",
        "display(im)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
