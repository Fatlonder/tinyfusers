{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers transformers accelerate"
      ],
      "metadata": {
        "id": "eMQj4oa55D7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBh9qwXelh1t"
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "pipe.to(\"cuda\")\n",
        "prompt = \"a photograph of an astronaut riding a horse\"\n",
        "prompt = \"a horse sized cat eating a bagel\"\n",
        "num_steps = 20\n",
        "\n",
        "image = pipe(prompt, num_inference_steps=num_steps, height=512, width=512).images[0]\n",
        "display(image)"
      ],
      "metadata": {
        "id": "oXQWW39e6WfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/tinygrad/tinygrad.git\n",
        "!pip install git+https://github.com/Fatlonder/tinyfusers.git"
      ],
      "metadata": {
        "id": "UCTtJbak_O_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tempfile\n",
        "from tinygrad import Device, GlobalCounters, dtypes, Tensor, TinyJit\n",
        "from tinygrad.nn.state import torch_load, load_state_dict, get_state_dict\n",
        "from tinygrad.helpers import Timing, Context, getenv, fetch, colored\n",
        "from tinyfusers.variants.sd import StableDiffusion\n",
        "from tinyfusers.tokenizer.clip import ClipTokenizer"
      ],
      "metadata": {
        "id": "MmB_-X46-auZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "default_prompt = \"a horse sized cat eating a bagel\"\n",
        "args = {\"prompt\": default_prompt, \"steps\": 20, \"fp16\": True, \"out\": \"rendered.png\", \"noshow\": False, \"timing\": False, \"guidance\":7.5, \"seed\": 42}\n",
        "Tensor.no_grad = True\n",
        "model = StableDiffusion()"
      ],
      "metadata": {
        "id": "YLC4Dyfq-eaY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_state_dict(model, torch_load(fetch('https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt', 'sd-v1-4.ckpt'))['state_dict'], strict=False)"
      ],
      "metadata": {
        "id": "ET1_qoi0-gLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for l in get_state_dict(model).values():\n",
        "  l.replace(l.cast(dtypes.float16).realize())"
      ],
      "metadata": {
        "id": "D5DEPfg7-hkY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run through CLIP to get context\n",
        "tokenizer = ClipTokenizer()\n",
        "prompt = Tensor([tokenizer.encode(args['prompt'])])\n",
        "context = model.cond_stage_model.transformer.text_model(prompt).realize()\n",
        "print(\"got CLIP context\", context.shape)\n",
        "\n",
        "prompt = Tensor([tokenizer.encode(\"\")])\n",
        "unconditional_context = model.cond_stage_model.transformer.text_model(prompt).realize()\n",
        "print(\"got unconditional CLIP context\", unconditional_context.shape)\n",
        "\n",
        "# done with clip model\n",
        "del model.cond_stage_model"
      ],
      "metadata": {
        "id": "AIT8baU7-wtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps = list(range(1, 1000, 1000//args['steps']))\n",
        "print(f\"running for {timesteps} timesteps\")\n",
        "alphas = model.alphas_cumprod[Tensor(timesteps)]\n",
        "alphas_prev = Tensor([1.0]).cat(alphas[:-1])\n",
        "\n",
        "# start with random noise\n",
        "if args['seed'] is not None: Tensor._seed = args['seed']\n",
        "latent = Tensor.randn(1,4,64,64)\n",
        "\n",
        "@TinyJit\n",
        "def run(model, *x): return model(*x).realize()\n",
        "\n",
        "# this is diffusion\n",
        "with Context(BEAM=getenv(\"LATEBEAM\")):\n",
        "  for index, timestep in (t:=tqdm(list(enumerate(timesteps))[::-1])):\n",
        "    GlobalCounters.reset()\n",
        "    t.set_description(\"%3d %3d\" % (index, timestep))\n",
        "    with Timing(\"step in \", enabled=args['timing'], on_exit=lambda _: f\", using {GlobalCounters.mem_used/1e9:.2f} GB\"):\n",
        "      tid = Tensor([index])\n",
        "      latent = run(model, unconditional_context, context, latent, Tensor([timestep]), alphas[tid], alphas_prev[tid], Tensor([args['guidance']]))\n",
        "      if args['timing']: Device[Device.DEFAULT].synchronize()\n",
        "  del run\n",
        "\n",
        "# upsample latent space to image with autoencoder\n",
        "x = model.decode(latent)\n",
        "print(x.shape)\n",
        "\n",
        "im = Image.fromarray(x.numpy().astype(np.uint8, copy=False))\n",
        "im.save(args['out'])\n",
        "display(im)"
      ],
      "metadata": {
        "id": "nSvvI1Ps-z_N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}