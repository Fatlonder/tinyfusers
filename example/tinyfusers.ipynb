{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMQj4oa55D7B"
      },
      "outputs": [],
      "source": [
        "get_ipython().system('nvidia-smi')\n",
        "get_ipython().system('pip install nvidia-cudnn-cu12')\n",
        "get_ipython().system('CUDNN_PATH=`pip show nvidia-cudnn-cu12  | grep Location | cut -d\":\" -f2 | xargs`/nvidia/cudnn pip install git+https://github.com/NVIDIA/cudnn-frontend.git')\n",
        "#get_ipython().system('pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121')\n",
        "!pip install diffusers transformers accelerate\n",
        "!pip install cupy-cuda12x pillow\n",
        "!git clone --depth 1 --branch v0.8.0 https://github.com/tinygrad/tinygrad\n",
        "!pip install -e git+https://github.com/Fatlonder/tinyfusers.git .\n",
        "\n",
        "%cd tinygrad\n",
        "!rm -r build\n",
        "!pip install -e .\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXQWW39e6WfG"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "from diffusers import StableDiffusionPipeline\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
        "\n",
        "pipe.to(\"cuda\")\n",
        "prompt = \"a photograph of an astronaut riding a horse\"\n",
        "prompt = \"a horse sized cat eating a bagel\"\n",
        "num_steps = 20\n",
        "\n",
        "image = pipe(prompt, num_inference_steps=num_steps, height=512, width=512).images[0]\n",
        "display(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MmB_-X46-auZ"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, 'tinyfusers')\n",
        "\n",
        "import tinyfusers\n",
        "import importlib\n",
        "importlib.reload(tinyfusers)\n",
        "print(tinyfusers.__file__)\n",
        "#get_ipython().kernel.do_shutdown(restart=True)\n",
        "\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "from tinygrad import Device, GlobalCounters, dtypes, Tensor, TinyJit\n",
        "from tinygrad.nn.state import torch_load, load_state_dict, get_state_dict\n",
        "from tinygrad.helpers import Timing, Context, getenv, fetch, colored\n",
        "from tinyfusers.variants.sd import StableDiffusion\n",
        "from tinyfusers.tokenizer.clip import ClipTokenizer\n",
        "import gc\n",
        "import ctypes\n",
        "libc = ctypes.CDLL(\"libc.so.6\")\n",
        "mempool = cp.get_default_memory_pool()\n",
        "pinned_mempool = cp.get_default_pinned_memory_pool()\n",
        "\n",
        "def model_to_fp16(model):\n",
        "  for l in get_state_dict(model).values():\n",
        "    l.replace(l.cast(dtypes.float16).realize())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YLC4Dyfq-eaY"
      },
      "outputs": [],
      "source": [
        "default_prompt = \"a horse sized cat eating a bagel\"\n",
        "args = {\"prompt\": default_prompt, \"steps\": 20, \"fp16\": True, \"out\": \"rendered.png\", \"noshow\": False, \"timing\": False, \"guidance\":7.5, \"seed\": 42}\n",
        "Tensor.no_grad = True\n",
        "model = StableDiffusion()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ET1_qoi0-gLC"
      },
      "outputs": [],
      "source": [
        "load_state_dict(model, torch_load(fetch('https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt', 'sd-v1-4.ckpt'))['state_dict'], strict=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIT8baU7-wtF"
      },
      "outputs": [],
      "source": [
        "# run through CLIP to get context\n",
        "tokenizer = ClipTokenizer()\n",
        "prompt = Tensor([tokenizer.encode(args['prompt'])])\n",
        "context = model.cond_stage_model.transformer.text_model(prompt).realize()\n",
        "print(\"got CLIP context\", context.shape)\n",
        "\n",
        "prompt = Tensor([tokenizer.encode(\"\")])\n",
        "unconditional_context = model.cond_stage_model.transformer.text_model(prompt).realize()\n",
        "print(\"got unconditional CLIP context\", unconditional_context.shape)\n",
        "\n",
        "# done with clip model\n",
        "del model.cond_stage_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSvvI1Ps-z_N"
      },
      "outputs": [],
      "source": [
        "timesteps = list(range(1, 1000, 1000//args['steps']))\n",
        "print(f\"running for {timesteps} timesteps\")\n",
        "alphas = model.alphas_cumprod[Tensor(timesteps)]\n",
        "alphas_prev = Tensor([1.0]).cat(alphas[:-1])\n",
        "\n",
        "# start with random noise\n",
        "if args['seed'] is not None: Tensor._seed = args['seed']\n",
        "latent = Tensor.randn(1,4,64,64)\n",
        "\n",
        "@TinyJit\n",
        "def run(model, *x): return model(*x).realize()\n",
        "\n",
        "# this is diffusion\n",
        "with Context(BEAM=getenv(\"LATEBEAM\")):\n",
        "  for index, timestep in (t:=tqdm(list(enumerate(timesteps))[::-1])):\n",
        "    GlobalCounters.reset()\n",
        "    t.set_description(\"%3d %3d\" % (index, timestep))\n",
        "    with Timing(\"step in \", enabled=args['timing'], on_exit=lambda _: f\", using {GlobalCounters.mem_used/1e9:.2f} GB\"):\n",
        "      tid = Tensor([index])\n",
        "      latent = run(model, unconditional_context, context, latent, Tensor([timestep]), alphas[tid], alphas_prev[tid], Tensor([args['guidance']]))\n",
        "      if args['timing']: Device[Device.DEFAULT].synchronize()\n",
        "  del run\n",
        "\n",
        "# upsample latent space to image with autoencoder\n",
        "x = model.decode(latent)\n",
        "print(x.shape)\n",
        "\n",
        "im = Image.fromarray(x.numpy().astype(np.uint8, copy=False))\n",
        "#im.save(args['out'])\n",
        "display(im)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
